{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising Models with TensorBoard\n",
    "\n",
    "**This lesson is adapted from [sentdex](https://www.youtube.com/watch?v=lV09_8432VA) on YouTube. I have collated this for my own learning experience, as well as for the benefit of others who would like to learn as well. :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be looking at how to improve our models further by looking at the plotted graphs of our training and validation loss on TensorBoard. \n",
    "\n",
    "Many times, the performance of models can be significantly improved by changing for example, the number of layers or number of hidden nodes - basically almost anything. \n",
    "\n",
    "Furthermore, it is usualy difficult to predict the optimal number of layers and neurons that your model require without the help of a research paper or continuous trial and error. \n",
    "\n",
    "Hence, TensorBoard can help make that process easier by giving you a visual representation of how well your model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we specify the different numbers of dense layers, layer sizes, and convolutional 2D layers that we would like to test. \n",
    "\n",
    "For the layer_sizes, the actual numbers that you input actually does not matter much (you can input 30, 60, 120 and it's okay). \n",
    "\n",
    "But do take note that inputting extremely large numbers or a large quantity of numbers will evidently take a longer time to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_layers = [0, 1, 2]\n",
    "# layer_sizes = [32, 64, 128]\n",
    "# conv_layers = [1, 2, 3]\n",
    "\n",
    "dense_layers = [1]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to see what is the total number of different combinations we could possibly go ith. You would see that small changes from above would lead to a large number of different combinations that we could take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-conv-32-nodes-0-dense-1591430540\n",
      "2-conv-32-nodes-0-dense-1591430540\n",
      "3-conv-32-nodes-0-dense-1591430540\n",
      "1-conv-64-nodes-0-dense-1591430540\n",
      "2-conv-64-nodes-0-dense-1591430540\n",
      "3-conv-64-nodes-0-dense-1591430540\n",
      "1-conv-128-nodes-0-dense-1591430540\n",
      "2-conv-128-nodes-0-dense-1591430540\n",
      "3-conv-128-nodes-0-dense-1591430540\n",
      "1-conv-32-nodes-1-dense-1591430540\n",
      "2-conv-32-nodes-1-dense-1591430540\n",
      "3-conv-32-nodes-1-dense-1591430540\n",
      "1-conv-64-nodes-1-dense-1591430540\n",
      "2-conv-64-nodes-1-dense-1591430540\n",
      "3-conv-64-nodes-1-dense-1591430540\n",
      "1-conv-128-nodes-1-dense-1591430540\n",
      "2-conv-128-nodes-1-dense-1591430540\n",
      "3-conv-128-nodes-1-dense-1591430540\n",
      "1-conv-32-nodes-2-dense-1591430540\n",
      "2-conv-32-nodes-2-dense-1591430540\n",
      "3-conv-32-nodes-2-dense-1591430540\n",
      "1-conv-64-nodes-2-dense-1591430540\n",
      "2-conv-64-nodes-2-dense-1591430540\n",
      "3-conv-64-nodes-2-dense-1591430540\n",
      "1-conv-128-nodes-2-dense-1591430540\n",
      "2-conv-128-nodes-2-dense-1591430540\n",
      "3-conv-128-nodes-2-dense-1591430540\n"
     ]
    }
   ],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir = 'logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pickle.load(open(\"x.pickle\", \"rb\"))\n",
    "y = pickle.load(open(\"y.pickle\", \"rb\"))\n",
    "\n",
    "#normalise the data\n",
    "#since the maximum pixel value is 255, we will divide our x by 255 to achieve values between 0 and 1\n",
    "x = x/255.0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will be unusually long, but I will try to explain each step as much as I can along the code lines. \n",
    "\n",
    "In summary, what we are doing is that we first loop through dense layers, layer sizes and convolutional layers. Do let me know if I am wrong but, there should not be a difference in which order the three factors are looped through first. By doing so, we will run the model through every possible combination that we have seen earlier, and ideally pick the best performing model from TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-1-dense-1591517560\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/10\n",
      "17462/17462 [==============================] - 18s 1ms/sample - loss: 0.6596 - accuracy: 0.5962 - val_loss: 0.5936 - val_accuracy: 0.6825\n",
      "Epoch 2/10\n",
      "17462/17462 [==============================] - 14s 795us/sample - loss: 0.5613 - accuracy: 0.7070 - val_loss: 0.5082 - val_accuracy: 0.7527\n",
      "Epoch 3/10\n",
      "17462/17462 [==============================] - 16s 925us/sample - loss: 0.4995 - accuracy: 0.7563 - val_loss: 0.4612 - val_accuracy: 0.7863\n",
      "Epoch 4/10\n",
      "17462/17462 [==============================] - 39s 2ms/sample - loss: 0.4455 - accuracy: 0.7929 - val_loss: 0.4634 - val_accuracy: 0.7854\n",
      "Epoch 5/10\n",
      "17462/17462 [==============================] - 54s 3ms/sample - loss: 0.4087 - accuracy: 0.8114 - val_loss: 0.4501 - val_accuracy: 0.7910\n",
      "Epoch 6/10\n",
      "17462/17462 [==============================] - 54s 3ms/sample - loss: 0.3668 - accuracy: 0.8370 - val_loss: 0.4522 - val_accuracy: 0.7934\n",
      "Epoch 7/10\n",
      "17462/17462 [==============================] - 54s 3ms/sample - loss: 0.3330 - accuracy: 0.8520 - val_loss: 0.4443 - val_accuracy: 0.8009\n",
      "Epoch 8/10\n",
      "17462/17462 [==============================] - 54s 3ms/sample - loss: 0.2986 - accuracy: 0.8702 - val_loss: 0.4207 - val_accuracy: 0.8168\n",
      "Epoch 9/10\n",
      "17462/17462 [==============================] - 55s 3ms/sample - loss: 0.2591 - accuracy: 0.8875 - val_loss: 0.6376 - val_accuracy: 0.7569\n",
      "Epoch 10/10\n",
      "17462/17462 [==============================] - 54s 3ms/sample - loss: 0.2201 - accuracy: 0.9066 - val_loss: 0.4773 - val_accuracy: 0.8148\n"
     ]
    }
   ],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            #identify the name of the combination for easier identificaiton later on\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir = 'logs\\{}'.format(NAME))\n",
    "            print(NAME)\n",
    "            \n",
    "            #using the sequential model\n",
    "            model = Sequential()\n",
    "\n",
    "            #first layer\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape = x.shape[1:]))\n",
    "            model.add(Activation(\"relu\"))\n",
    "            model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "            \n",
    "            #we will only execute the 2nd and third layers of the convolutional layers when conv_layers > 1\n",
    "            for l in range(conv_layer - 1):\n",
    "                #second layer\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation(\"relu\"))\n",
    "                model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "                \n",
    "            #third layer\n",
    "            #first we need to flatten the dataset because conv2d pass through a 2D dataset, whereas dense accepts a 1D dataset\n",
    "            model.add(Flatten())\n",
    "            \n",
    "            #we only execute the 1st and 2nd layers of dense layers if dense_layers > 0\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(512))\n",
    "                model.add(Activation('relu'))\n",
    "                \n",
    "            #output layer with sigmoid function as activation function\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            #specify the optimizer, loss function and metrics(optional)\n",
    "            #loss function here is binary crossentropy because we only have 2 outputs (cat or dog)\n",
    "            model.compile(loss = \"binary_crossentropy\",\n",
    "                         optimizer = \"adam\",\n",
    "                         metrics = [\"accuracy\"])\n",
    "\n",
    "            model.fit(x, y, batch_size = 32, epochs = 10, validation_split = 0.3, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to view your graphs on tensorboard, open your command prompt and navigate to the directory where your logs file is located. Then, input the following command:\n",
    "\n",
    "*tensorboard --logdir=logs/*\n",
    "\n",
    "Copy and paste the link outputted into your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toggle to see all your validation losses to pick the best performing model (it should be the lowest graph). For me, my best performing model was 3-conv-64-nodes-0-dense. It could be a little different for you, but the consensus is that 3 conv layers and 0 dense layers are often the better models.\n",
    "\n",
    "Next, we should see if we could improve our models better by inputting other different values while keeping the well-performing values constant. So for example, I kept my conv layers at 3, and decide to test out if a larger number of neurons within a dense layer could allow the model to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-and-keras-tutorial",
   "language": "python",
   "name": "tensorflow-and-keras-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
